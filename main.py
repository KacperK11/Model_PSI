
print("start")
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt # Dodano dla pewno≈õci

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# Nowe modele do por√≥wnania
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Wszystkie potrzebne metryki
from sklearn.metrics import (
    accuracy_score, 
    f1_score, 
    roc_auc_score, 
    classification_report, 
    recall_score, 
    precision_score,
    confusion_matrix,
    ConfusionMatrixDisplay,
    roc_curve,
    auc,
    precision_recall_curve
)

# === SMOTE + imblearn pipeline ===
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE

# =========================
# 1. Wczytanie danych
# =========================
print("Wczytywanie danych...")
df = pd.read_csv("healthcare-dataset-stroke-data.csv")


y = df["stroke"]

numeric_features = [
    "age",
    "avg_glucose_level",
    "bmi",
    "hypertension",
    "heart_disease"
]

categorical_features = [
    "gender",
    "ever_married",
    "work_type",
    "Residence_type",
    "smoking_status"
]

X = df[numeric_features + categorical_features]


# =========================
# 2. Preprocessing (Pipeline)
# =========================
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)


# =========================
# 3. Podzia≈Ç train / test
# =========================
X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# ==========================================
# 3a. MODU≈Å POR√ìWNANIA MODELI (DO RAPORTU)
# ==========================================
print("\n=== ROZPOCZYNAM POR√ìWNANIE R√ì≈ªNYCH MODELI ===")
print("(To pozwoli wype≈Çniƒá tabelƒô w Sekcji 5 raportu)\n")

models_to_test = [
    ("Logistic Regression", LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)),
    ("SVM (RBF)", SVC(class_weight='balanced', probability=True, random_state=42)),
    ("Random Forest (Base)", RandomForestClassifier(class_weight='balanced', random_state=42))
]

for name, model in models_to_test:
    # Tworzymy tymczasowy pipeline dla ka≈ºdego modelu
    temp_pipeline = Pipeline(steps=[
        ("preprocessor", preprocessor),
        ("classifier", model)
    ])
    
    print(f"Trenowanie: {name}...")
    temp_pipeline.fit(X_train, y_train)
    
    # Predykcja
    y_pred_temp = temp_pipeline.predict(X_test)
    # SVM i LogReg mogƒÖ wymagaƒá probability=True dla predict_proba
    try:
        y_prob_temp = temp_pipeline.predict_proba(X_test)[:, 1]
        roc_auc_val = roc_auc_score(y_test, y_prob_temp)
    except:
        roc_auc_val = 0.5 # Fallback je≈õli model nie obs≈Çuguje prawdopodobie≈Ñstw
    
    # Wypisanie wynik√≥w w formacie gotowym do przepisania
    print(f"--- WYNIKI: {name} ---")
    print(f"Accuracy:  {accuracy_score(y_test, y_pred_temp):.3f}")
    print(f"Recall:    {recall_score(y_test, y_pred_temp):.3f}")
    print(f"Precision: {precision_score(y_test, y_pred_temp):.3f}")
    print(f"ROC AUC:   {roc_auc_val:.3f}")
    print("-" * 30)

print("\n=== KONIEC POR√ìWNANIA. PRZECHODZƒò DO OPTYMALIZACJI RANDOM FOREST ===\n")


# =========================
# 4. Pipeline + Random Forest (G≈Ç√≥wna czƒô≈õƒá projektu)
# =========================



pipeline = ImbPipeline(steps=[
    ("preprocessor", preprocessor),
    ("smote", SMOTE(random_state=42)),
    ("model", RandomForestClassifier(
        random_state=42,
        class_weight="balanced"
    ))
])


# =========================
# 5. GridSearchCV
# =========================
param_grid = {
    "model__n_estimators": [100, 200],
    "model__max_depth": [None, 10, 20],
    "model__min_samples_split": [2, 5],
    "model__min_samples_leaf": [1, 2]
}

grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=5,
    scoring="roc_auc",
    n_jobs=-1,
    verbose=2
)

print("\nUruchamiam GridSearch (to potrwa chwilƒô)...")
grid_search.fit(X_train, y_train)


# =========================
# 6. Najlepszy model
# =========================
print("\nNajlepsze hiperparametry:")
print(grid_search.best_params_)

best_model = grid_search.best_estimator_


# =========================
# 7. Ocena na zbiorze testowym
# =========================
from sklearn.metrics import precision_recall_curve, auc

y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)[:, 1]

acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc = roc_auc_score(y_test, y_prob)

# === DODANE: PR-AUC ===
precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_prob)
pr_auc = auc(recall_vals, precision_vals)
# =======================

print("\nWYNIKI NA ZBIORZE TESTOWYM:")
print(f"Accuracy: {acc:.3f}")
print(f"F1-score: {f1:.3f}")
print(f"ROC AUC: {roc:.3f}")
print(f"PR AUC: {pr_auc:.3f}")  # <= wypisanie PR-AUC

print("\nClassification report:")
print(classification_report(y_test, y_pred))



# ============================================================
# EKSPERYMENT NAPRAWCZY:
# GridSearch (recall) + zmiana progu decyzyjnego
# ============================================================

from sklearn.metrics import recall_score

print("\n================ EKSPERYMENT NAPRAWCZY ================\n")

# GridSearch nastawiony na RECALL (bez SMOTE)
param_grid_recall = {
    "model__n_estimators": [100, 200],
    "model__max_depth": [10, 20],
    "model__min_samples_split": [2, 5],
    "model__min_samples_leaf": [1, 2]
}

grid_search_recall = GridSearchCV(
    pipeline,              # <-- TEN pipeline ju≈º masz
    param_grid_recall,
    cv=5,
    scoring="recall",
    n_jobs=-1,
    verbose=2
)

print("Uruchamiam GridSearch (optymalizacja Recall)...")
grid_search_recall.fit(X_train, y_train)

print("\nNajlepsze hiperparametry (Recall):")
print(grid_search_recall.best_params_)

best_recall_model = grid_search_recall.best_estimator_

# ============================================================
# ZMIANA PROGU DECYZYJNEGO
# ============================================================

y_prob = best_recall_model.predict_proba(X_test)[:, 1]

print("\n--- Wyniki dla r√≥≈ºnych prog√≥w decyzyjnych ---\n")

for threshold in [0.5, 0.3, 0.2]:
    y_pred_thresh = (y_prob >= threshold).astype(int)

    acc = accuracy_score(y_test, y_pred_thresh)
    recall = recall_score(y_test, y_pred_thresh)
    f1 = f1_score(y_test, y_pred_thresh)

    print(f"Pr√≥g decyzyjny = {threshold}")
    print(f"Accuracy: {acc:.3f}")
    print(f"Recall (udar): {recall:.3f}")
    print(f"F1-score: {f1:.3f}")
    print(classification_report(y_test, y_pred_thresh))
    print("-" * 60)

# ============================================================
# SEKCJA: OCENA JAKO≈öCI MODELU I ANALIZA PROGU DECYZYJNEGO
#
# W tej sekcji przeprowadzono wizualnƒÖ ocenƒô jako≈õci najlepszego
# modelu po eksperymentach naprawczych (GridSearch z optymalizacjƒÖ
# pod kƒÖtem miary recall).
#
# Analiza obejmuje:
# 1) KrzywƒÖ ROC oraz warto≈õƒá ROC AUC ‚Äì ocenƒô zdolno≈õci modelu
#    do rozr√≥≈ºniania pacjent√≥w z udarem i bez udaru niezale≈ºnie
#    od progu decyzyjnego.
# 2) KrzywƒÖ Precision‚ÄìRecall ‚Äì kluczowƒÖ dla niezbalansowanych
#    zbior√≥w danych, ilustrujƒÖcƒÖ kompromis pomiƒôdzy precyzjƒÖ
#    a czu≈Ço≈õciƒÖ klasy pozytywnej.
# 3) Macierze pomy≈Çek dla r√≥≈ºnych prog√≥w decyzyjnych, pokazujƒÖce
#    wp≈Çyw zmiany progu na liczbƒô fa≈Çszywie pozytywnych oraz
#    fa≈Çszywie negatywnych decyzji.
# ============================================================

import matplotlib.pyplot as plt
from sklearn.metrics import (
    roc_curve,
    precision_recall_curve,
    auc,
    ConfusionMatrixDisplay
)

# Pobierz prawdopodobie≈Ñstwa dla klasy pozytywnej (udar)
y_scores = best_recall_model.predict_proba(X_test)[:, 1]

# ============================================================
# 1. KRZYWA ROC (Receiver Operating Characteristic)
# ============================================================

from sklearn.metrics import roc_curve, auc

fpr, tpr, _ = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 5))
plt.plot(
    fpr,
    tpr,
    color="darkorange",
    lw=2,
    label=f"ROC curve (AUC = {roc_auc:.2f})"
)
plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate (Recall)")
plt.title("Krzywa ROC")
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.savefig("roc_curve.png", dpi=300)
plt.show()
plt.close()


# ============================================================
# 2. KRZYWA PRECISION‚ÄìRECALL
# ============================================================

from sklearn.metrics import precision_recall_curve

precision, recall, _ = precision_recall_curve(y_test, y_scores)

plt.figure(figsize=(6, 5))
plt.plot(recall, precision, lw=2)
plt.xlabel("Recall (Pe≈Çno≈õƒá)")
plt.ylabel("Precision (Precyzja)")
plt.title("Krzywa Precision‚ÄìRecall")
plt.grid(True)
plt.tight_layout()
plt.savefig("precision_recall_curve.png", dpi=300)
plt.show()
plt.close()
# ============================================================
# 3. MACIERZE POMY≈ÅEK DLA R√ì≈ªNYCH PROG√ìW DECYZYJNYCH
# ============================================================

thresholds_to_show = [0.5, 0.2]

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

for i, thresh in enumerate(thresholds_to_show):
    # Klasyfikacja przy u≈ºyciu niestandardowego progu decyzyjnego
    y_pred_custom = (y_scores >= thresh).astype(int)

    ConfusionMatrixDisplay.from_predictions(
        y_test,
        y_pred_custom,
        ax=axes[i],
        colorbar=False,
        cmap="Blues"
    )

    axes[i].set_title(f"Macierz pomy≈Çek (Pr√≥g = {thresh})")

plt.tight_layout()
plt.savefig("macierze_pomylek.png")
plt.show()

# ============================================================
# ROZSZERZENIE PROJEKTU: ANALIZA INTERPRETOWALNO≈öCI I JAKO≈öCI
# ============================================================
# W tej sekcji przeprowadzono dodatkowe eksperymenty majƒÖce na celu:
# 1) interpretacjƒô dzia≈Çania modelu (feature importance),
# 2) analizƒô wp≈Çywu progu decyzyjnego na jako≈õƒá predykcji,
# 3) ocenƒô koszt√≥w b≈Çƒôd√≥w klasyfikacyjnych w kontek≈õcie medycznym.
# ============================================================


# ============================================================
# üîç SEKCJA 2: ANALIZA WA≈ªNO≈öCI CECH (FEATURE IMPORTANCE)
# ============================================================

import pandas as pd
import matplotlib.pyplot as plt

print("\n>>> START FEATURE IMPORTANCE <<<")

# ------------------------------------------------------------
# 1. WyciƒÖgniƒôcie RandomForest i preprocessora z Pipeline
# ------------------------------------------------------------

rf_model = best_model.named_steps["model"]
preprocessor = best_model.named_steps["preprocessor"]

# ------------------------------------------------------------
# 2. Nazwy cech po OneHotEncoding + cechy numeryczne
# ------------------------------------------------------------

feature_names = preprocessor.get_feature_names_out()

# ------------------------------------------------------------
# 3. Tabela wa≈ºno≈õci cech
# ------------------------------------------------------------

feature_importance_df = pd.DataFrame({
    "Cecha": feature_names,
    "Wa≈ºno≈õƒá": rf_model.feature_importances_
}).sort_values(by="Wa≈ºno≈õƒá", ascending=False)

print("\nTOP 10 najwa≈ºniejszych cech wg Random Forest:")
print(feature_importance_df.head(10))

# ------------------------------------------------------------
# 4. Wykres TOP 10
# ------------------------------------------------------------

plt.figure(figsize=(10, 6))
plt.barh(
    feature_importance_df.head(10)["Cecha"][::-1],
    feature_importance_df.head(10)["Wa≈ºno≈õƒá"][::-1]
)
plt.xlabel("Wa≈ºno≈õƒá cechy")
plt.title("Najwa≈ºniejsze cechy w predykcji udaru (Random Forest)")
plt.tight_layout()
plt.savefig("feature_importance.png")
plt.show()


# ============================================================
# üìâ SEKCJA 3: WP≈ÅYW PROGU DECYZYJNEGO NA RECALL (PE≈ÅNO≈öƒÜ)
# ============================================================

from sklearn.metrics import recall_score

thresholds = np.linspace(0.01, 0.99, 50)
recall_values = []

for t in thresholds:
    y_pred_t = (y_scores >= t).astype(int)
    recall_values.append(recall_score(y_test, y_pred_t))

plt.figure(figsize=(8, 5))
plt.plot(thresholds, recall_values, marker="o")
plt.xlabel("Pr√≥g decyzyjny")
plt.ylabel("Recall (Pe≈Çno≈õƒá) ‚Äì klasa udaru")
plt.title("Wp≈Çyw progu decyzyjnego na Recall")
plt.grid(True)
plt.tight_layout()
plt.savefig("recall_vs_threshold.png")
plt.show()


# ============================================================
# ‚öñÔ∏è SEKCJA 4: ANALIZA KOSZT√ìW B≈ÅƒòD√ìW (FN vs FP)
# ============================================================

from sklearn.metrics import confusion_matrix

thresholds_to_check = [0.5, 0.3, 0.2]

print("\nAnaliza koszt√≥w b≈Çƒôd√≥w klasyfikacji:\n")

for t in thresholds_to_check:
    y_pred_t = (y_scores >= t).astype(int)
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_t).ravel()

    print(f"Pr√≥g decyzyjny = {t}")
    print(f"TP (poprawnie wykryte udary): {tp}")
    print(f"FN (przegapione udary):     {fn}")
    print(f"FP (fa≈Çszywe alarmy):       {fp}")
    print(f"TN (poprawnie zdrowi):      {tn}")
    print("-" * 50)


# ============================================================
# üîç SEKCJA: INTERPRETOWALNO≈öƒÜ MODELU ‚Äì FEATURE IMPORTANCE
#
# Celem tej sekcji jest interpretacja dzia≈Çania najlepszego
# modelu (Random Forest) poprzez analizƒô wa≈ºno≈õci cech.
#
# Random Forest umo≈ºliwia oszacowanie, kt√≥re zmienne wej≈õciowe
# mia≈Çy najwiƒôkszy wp≈Çyw na podejmowane decyzje klasyfikacyjne.
# Jest to szczeg√≥lnie istotne w zastosowaniach medycznych,
# gdzie interpretowalno≈õƒá modelu ma kluczowe znaczenie.
# ============================================================
print(">>> START FEATURE IMPORTANCE <<<")
import pandas as pd
import matplotlib.pyplot as plt

# ------------------------------------------------------------
# 1. Pobranie najlepszego wytrenowanego modelu z GridSearch
# ------------------------------------------------------------
# GridSearchCV zawiera pipeline, dlatego:
# - 'model' to RandomForestClassifier
# - 'preprocessor' to etap przetwarzania danych
# ------------------------------------------------------------

rf_model = grid_search.best_estimator_.named_steps["model"]
preprocessor = grid_search.best_estimator_.named_steps["preprocessor"]

# ------------------------------------------------------------
# 2. Pobranie nazw cech po transformacji
# ------------------------------------------------------------
# OneHotEncoder tworzy nowe kolumny, dlatego korzystamy
# z metody get_feature_names_out(), aby uzyskaƒá pe≈ÇnƒÖ listƒô
# cech wej≈õciowych po preprocessingu.
# ------------------------------------------------------------

feature_names = preprocessor.get_feature_names_out()

# ------------------------------------------------------------
# 3. Utworzenie tabeli wa≈ºno≈õci cech
# ------------------------------------------------------------

importances = pd.DataFrame({
    "Cecha": feature_names,
    "Wa≈ºno≈õƒá": rf_model.feature_importances_
})

# Sortowanie i wyb√≥r 10 najwa≈ºniejszych cech
importances = importances.sort_values(
    by="Wa≈ºno≈õƒá",
    ascending=False
).head(10)

# ------------------------------------------------------------
# 4. Wizualizacja ‚Äì wykres wa≈ºno≈õci cech
# ------------------------------------------------------------

plt.figure(figsize=(10, 6))
plt.barh(
    importances["Cecha"],
    importances["Wa≈ºno≈õƒá"],
    color="skyblue"
)
plt.xlabel("Wa≈ºno≈õƒá cechy (Feature Importance)")
plt.title("Najwa≈ºniejsze cechy wp≈ÇywajƒÖce na ryzyko udaru (Random Forest)")
plt.gca().invert_yaxis()  # Najwa≈ºniejsze cechy na g√≥rze
plt.tight_layout()
plt.savefig("waznosc_cech.png")
plt.show()

# ------------------------------------------------------------
# 5. Wy≈õwietlenie tabeli z wynikami
# ------------------------------------------------------------

print("\nNajwa≈ºniejsze cechy wg modelu Random Forest:")
print(importances)

# ==========================================
# ANALIZA EDA DO RAPORTU
# ==========================================
print("--- 1. WYMIARY ZBIORU ---")
print(f"Liczba wszystkich rekord√≥w (pacjent√≥w): {df.shape[0]}")
print(f"Liczba kolumn (cech): {df.shape[1]}")

print("\n--- 2. ROZK≈ÅAD KLASY DOCELOWEJ (STROKE) ---")
counts = df["stroke"].value_counts()
percents = df["stroke"].value_counts(normalize=True) * 100

print(f"Brak udaru (0): {counts[0]} os√≥b ({percents[0]:.2f}%)")
print(f"Udar (1):       {counts[1]} os√≥b ({percents[1]:.2f}%)")

print("\n--- 3. BRAKI DANYCH ---")
missing_bmi = df["bmi"].isnull().sum()
missing_percent = (missing_bmi / df.shape[0]) * 100
print(f"Braki w kolumnie BMI: {missing_bmi} ({missing_percent:.1f}%)")

# Sprawdzenie czy sƒÖ braki gdziekolwiek indziej
total_missing = df.isnull().sum()
print("\nPe≈Çna mapa brak√≥w (dla pewno≈õci):")
print(total_missing[total_missing > 0])

